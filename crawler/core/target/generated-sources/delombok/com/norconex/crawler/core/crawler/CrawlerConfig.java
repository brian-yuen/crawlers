// Generated by delombok at Fri Mar 08 16:24:33 MST 2024
/* Copyright 2014-2023 Norconex Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.norconex.crawler.core.crawler;

import java.nio.file.Path;
import java.time.Duration;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;
import com.fasterxml.jackson.annotation.JsonIgnore;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.dataformat.xml.annotation.JacksonXmlElementWrapper;
import com.fasterxml.jackson.dataformat.xml.annotation.JacksonXmlProperty;
import com.norconex.committer.core.Committer;
import com.norconex.commons.lang.collection.CollectionUtil;
import com.norconex.commons.lang.event.EventListener;
import com.norconex.crawler.core.checksum.DocumentChecksummer;
import com.norconex.crawler.core.checksum.MetadataChecksummer;
import com.norconex.crawler.core.checksum.impl.MD5DocumentChecksummer;
import com.norconex.crawler.core.doc.CrawlDocMetadata;
import com.norconex.crawler.core.fetch.FetchDirectiveSupport;
import com.norconex.crawler.core.fetch.Fetcher;
import com.norconex.crawler.core.filter.DocumentFilter;
import com.norconex.crawler.core.filter.MetadataFilter;
import com.norconex.crawler.core.filter.ReferenceFilter;
import com.norconex.crawler.core.processor.DocumentProcessor;
import com.norconex.crawler.core.spoil.SpoiledReferenceStrategizer;
import com.norconex.crawler.core.spoil.impl.GenericSpoiledReferenceStrategizer;
import com.norconex.crawler.core.store.DataStoreEngine;
import com.norconex.crawler.core.store.impl.mvstore.MVStoreDataStoreEngine;
import com.norconex.importer.ImporterConfig;

/**
 * <p>
 * Base Crawler configuration. Crawlers usually read this configuration upon
 * starting up.  Once execution has started, it should not be changed
 * to avoid unexpected behaviors.
 * </p>
 *
 * <p>
 * Concrete implementations inherit the following XML configuration
 * options (typically within a <code>&lt;crawler&gt;</code> tag):
 * </p>
 *
 * {@nx.xml #init
 *
 *   <numThreads>(maximum number of threads)</numThreads>
 *   <maxDocuments>
 *     (maximum number of documents to crawl per session, resuming on next
 *      sessions where it last ended, if crawling was not complete)
 *   </maxDocuments>
 *   <maxDepth>(maximum depth the crawler should go)</maxDepth>
 *   <idleTimeout>(thread inactivity timeout)</idleTimeout>
 *   <minProgressLoggingInterval>
 *     (minimum frequency at which progress is logged)
 *   </minProgressLoggingInterval>
 *   <orphansStrategy>[PROCESS|IGNORE|DELETE]</orphansStrategy>
 *
 *   <stopOnExceptions>
 *     <!-- Repeatable -->
 *     <exception>(fully qualified class name of a an exception)</exception>
 *   </stopOnExceptions>
 *
 *   <eventListeners>
 *     <!-- Repeatable -->
 *     <listener class="(EventListener implementation)"/>
 *   </eventListeners>
 *
 *   <dataStoreEngine class="(DataStoreEngine implementation)" />
 * }
 *
 * {@nx.xml #start-refs
 *   <start>
 *     <!-- All the following tags are repeatable. -->
 *     <ref>(a reference)</ref>
 *     <refsFile>(local path to a file containing references)</refsFile>
 *     <provider class="(StartRefsProvider implementation)"/>
 *   </start>
 * }
 *
 * {@nx.xml #fetchers
 *   <fetchers
 *       maxRetries="(number of times to retry a failed fetch attempt)"
 *       retryDelay="(how many milliseconds to wait between re-attempting)">
 *     <!-- Repeatable -->
 *     <fetcher class="(Fetcher implementation)"/>
 *   </fetchers>
 * }
 *
 * {@nx.xml #pipeline-queue
 *   <referenceFilters>
 *     <!-- Repeatable -->
 *     <filter
 *         class="(ReferenceFilter implementation)"
 *         onMatch="[include|exclude]" />
 *   </referenceFilters>
 * }
 *
 * {@nx.xml #pipeline-import
 *   <metadataFilters>
 *     <!-- Repeatable -->
 *     <filter
 *         class="(MetadataFilter implementation)"
 *         onMatch="[include|exclude]" />
 *   </metadataFilters>
 *
 *   <documentFilters>
 *     <!-- Repeatable -->
 *     <filter class="(DocumentFilter implementation)" />
 *   </documentFilters>
 * }
 *
 * {@nx.xml #import
 *   <importer>
 *     <preParseHandlers>
 *       <!-- Repeatable -->
 *       <handler class="(an handler class from the Importer module)"/>
 *     </preParseHandlers>
 *     <documentParserFactory class="(DocumentParser implementation)" />
 *     <postParseHandlers>
 *       <!-- Repeatable -->
 *       <handler class="(an handler class from the Importer module)"/>
 *     </postParseHandlers>
 *     <responseProcessors>
 *       <!-- Repeatable -->
 *       <responseProcessor
 *              class="(ImporterResponseProcessor implementation)" />
 *     </responseProcessors>
 *   </importer>
 * }
 *
 * {@nx.xml #directive-meta
 *   <metadataFetchSupport>[DISABLED|REQUIRED|OPTIONAL]</metadataFetchSupport>
 * }
 * {@nx.xml #directive-doc
 *   <documentFetchSupport>[REQUIRED|DISABLED|OPTIONAL]</documentFetchSupport>
 * }
 *
 * {@nx.xml #checksum-meta
 *   <metadataChecksummer class="(MetadataChecksummer implementation)" />
 * }
 *
 * {@nx.xml #dedup-meta
 *   <metadataDeduplicate>[false|true]</metadataDeduplicate>
 * }
 *
 * {@nx.xml #checksum-doc
 *   <documentChecksummer class="(DocumentChecksummer implementation)" />
 * }
 *
 * {@nx.xml #dedup-doc
 *   <documentDeduplicate>[false|true]</documentDeduplicate>
 * }
 *
 * {@nx.xml #pipeline-committer
 *   <spoiledReferenceStrategizer
 *       class="(SpoiledReferenceStrategizer implementation)" />
 *
 *   <committers>
 *     <committer class="(Committer implementation)" />
 *   </committers>
 * }
 */
@SuppressWarnings("javadoc")
public class CrawlerConfig {

    public enum OrphansStrategy {
        /**
         * Processing orphans tries to obtain and process them again,
         * normally.
         */
        PROCESS, /**
         * Deleting orphans sends them to the Committer for deletions and
         * they are removed from the internal reference cache.
         */
        DELETE, /**
         * Ignoring orphans effectively does nothing with them
         * (not deleted, not processed).
         */
        IGNORE;
    }

    public static final Duration DEFAULT_IDLE_PROCESSING_TIMEOUT = Duration.ofMinutes(10);
    public static final Duration DEFAULT_MIN_PROGRESS_LOGGING_INTERVAL = Duration.ofSeconds(30);
    //--- Properties -----------------------------------------------------------
    /**
     * The crawler unique identifier.
     * Using usual names is perfectly fine (non-alphanumeric characters are OK).
     * It is important for this crawler ID to be unique amongst your
     * crawlers in the same crawl session. On top of avoiding conflicts,
     * it facilitates integration with different systems and facilitates
     * tracking.
     */
    @JsonProperty(required = true)
    private String id;
    private final List<String> startReferences = new ArrayList<>();
    private final List<Path> startReferencesFiles = new ArrayList<>();
    private final List<ReferencesProvider> startReferencesProviders = new ArrayList<>();
    /**
     * Whether the start references should be loaded asynchronously. When
     * <code>true</code>, the crawler will start processing the start
     * references in a separate thread as they are added to the queue
     * (as opposed to wait for queue initialization to be complete).
     * While this may speed up crawling, it may have an unexpected effect on
     * accuracy of {@link CrawlDocMetadata#DEPTH}. Use of this option is only
     * recommended when start references take a significant time to load.
     */
    private boolean startReferencesAsync;
    /**
     * The maximum number of threads a crawler can use. Default is two.
     */
    private int numThreads = 2;
    /**
     * <p>
     * The maximum number of documents that can be processed before stopping.
     * Not all processed documents make it to your Committers
     * as some can be rejected.
     * </p>
     * <p>
     * In multi-threaded or clustered environments, the actual number
     * of documents processed may be a bit higher than the specified
     * maximum due to concurrency.
     * Upon reaching the configured maximum, the crawler will finish with
     * its documents actively being processed before stopping.
     * </p>
     * <p>
     * Reaching the maximum value does not terminate the crawl session but
     * rather pauses it.  On next run, the crawler will resume the same session,
     * processing an additional number of documents up to the maximum
     * specified.
     * This maximum allows crawling one or more sources
     * in chunks, processing a maximum number of documents each time.
     * When the session fully completes, the next run will start a new
     * crawl session. To prevent resuming an partial crawl session,
     * explicitly clean the crawl session.
     * </p>
     * <p>
     * Default is -1 (unlimited).
     * </p>
     */
    private int maxDocuments = -1;
    /**
     * The maximum depth the crawler should go. The exact definition of depth
     * is crawler-specific. Examples: levels of sub-directories,
     * number of URL clicks to reach a page, etc. Refer to specific crawler
     * implementation for details. Default is -1 (unlimited).
     */
    private int maxDepth = -1;
    /**
     * The maximum amount of time to wait before shutting down an inactive
     * crawler thread.
     * A document taking longer to process than the specified timeout
     * when no other thread are available to process remaining documents
     * is also considered "inactive". Default is
     * {@value #DEFAULT_IDLE_PROCESSING_TIMEOUT}. A <code>null</code>
     * value means no timeouts.
     */
    private Duration idleTimeout;
    /**
     * Minimum amount of time to wait between each logging of crawling
     * progress.
     * Default value is {@value #DEFAULT_MIN_PROGRESS_LOGGING_INTERVAL}.
     * A <code>null</code> value disables progress logging. Minimum value
     * is 1 second.
     */
    private Duration minProgressLoggingInterval;
    /**
     * <p>The strategy to adopt when there are orphans.  Orphans are
     * references that were processed in a previous run, but were not in the
     * current run.  In other words, they are leftovers from a previous run
     * that were not re-encountered in the current.
     * </p><p>
     * Unless explicitly stated otherwise by an implementing class, the default
     * strategy is to <code>PROCESS</code> orphans.
     * Setting a <code>null</code> value is the same as setting
     * <code>IGNORE</code>.
     * </p><p>
     * <b>Be careful:</b> Setting the orphan strategy to <code>DELETE</code>
     * is NOT recommended in most cases. With some collectors, a temporary
     * failure such as a network outage or a web page timing out, may cause
     * some documents not to be crawled. When this happens, unreachable
     * documents would be considered "orphans" and be deleted while under
     * normal circumstances, they should be kept.  Re-processing them
     * (default), is usually the safest approach to confirm they still
     * exist before deleting or updating them.
     * </p>
     */
    private OrphansStrategy orphansStrategy = OrphansStrategy.PROCESS;
    private final List<Class<? extends Exception>> stopOnExceptions = new ArrayList<>();
    /**
     * The crawl data store factory.
     */
    private DataStoreEngine dataStoreEngine = new MVStoreDataStoreEngine();
    private final List<ReferenceFilter> referenceFilters = new ArrayList<>();
    private final List<MetadataFilter> metadataFilters = new ArrayList<>();
    private final List<DocumentFilter> documentFilters = new ArrayList<>();
    private final List<DocumentProcessor> preImportProcessors = new ArrayList<>();
    private final List<DocumentProcessor> postImportProcessors = new ArrayList<>();
    /**
     * The metadata checksummer.
     * Metadata checksum generation is disabled when <code>null</code>.
     */
    private MetadataChecksummer metadataChecksummer;
    /**
     * The Importer module configuration.
     */
    @JsonProperty("importer")
    private ImporterConfig importerConfig = new ImporterConfig();
    @JsonProperty("committers")
    @JacksonXmlElementWrapper(localName = "committers")
    @JacksonXmlProperty(localName = "committer")
    private final List<Committer> committers = new ArrayList<>();
    /**
     * Whether to turn on deduplication based on metadata checksum.
     * To enable, {@link #getMetadataChecksummer()} must not return
     * <code>null</code>.
     * Not recommended unless you know for sure your metadata
     * checksum is acceptably unique.
     */
    private boolean metadataDeduplicate;
    /**
     * Whether to turn on deduplication based on document checksum.
     * To enable, {@link #getDocumentChecksummer()} must not return
     * <code>null</code>.
     * Not recommended unless you know for sure your document
     * checksum is acceptably unique.
     */
    private boolean documentDeduplicate;
    /**
     * The document checksummer.
     * Document checksum generation is disabled when <code>null</code>.
     */
    private DocumentChecksummer documentChecksummer = new MD5DocumentChecksummer();
    /**
     * The spoiled state strategy resolver.
     */
    private SpoiledReferenceStrategizer spoiledReferenceStrategizer = new GenericSpoiledReferenceStrategizer();
    private final List<EventListener<?>> eventListeners = new ArrayList<>();
    private FetchDirectiveSupport metadataFetchSupport = FetchDirectiveSupport.DISABLED;
    private FetchDirectiveSupport documentFetchSupport = FetchDirectiveSupport.REQUIRED;
    /**
     * One or more fetchers responsible for obtaining documents and their
     * metadata from a source.
     * @param fetchers one or more fetchers
     * @return one or more fetchers
     */
    private final List<Fetcher<?, ?>> fetchers = new ArrayList<>();
    /**
     * The maximum number of times a fetcher will re-attempt fetching
     * a resource in case of failures.  Default is zero (won't retry).
     */
    private int fetchersMaxRetries;
    /**
     * How long to wait before a failing fetcher re-attempts fetching
     * a resource in case of failures (in milliseconds).
     * Default is zero (no delay).
     */
    private Duration fetchersRetryDelay;

    //--- List Accessors -------------------------------------------------------
    /**
     * Gets the references to initiate crawling from.
     * @return start references (never <code>null</code>)
     */
    public List<String> getStartReferences() {
        return Collections.unmodifiableList(startReferences);
    }

    /**
     * Sets the references to initiate crawling from.
     * @param startReferences start references
     */
    public CrawlerConfig setStartReferences(List<String> startReferences) {
        CollectionUtil.setAll(this.startReferences, startReferences);
        return this;
    }

    /**
     * Gets the file paths of seed files containing references to be used as
     * start references.  Files are expected to have one reference per line.
     * Blank lines and lines starting with # (comment) are ignored.
     * @return file paths of seed files containing references
     *         (never <code>null</code>)
     */
    public List<Path> getStartReferencesFiles() {
        return Collections.unmodifiableList(startReferencesFiles);
    }

    /**
     * Sets the file paths of seed files containing references to be used as
     * start references.  Files are expected to have one reference per line.
     * Blank lines and lines starting with # (comment) are ignored.
     * @param startReferencesFiles file paths of seed files containing
     *     references
     */
    public CrawlerConfig setStartReferencesFiles(List<Path> startReferencesFiles) {
        CollectionUtil.setAll(this.startReferencesFiles, startReferencesFiles);
        return this;
    }

    /**
     * Gets the providers of references used as starting points for crawling.
     * Use this approach when references need to be provided
     * dynamically at launch time.
     * @return start references providers (never <code>null</code>)
     */
    public List<ReferencesProvider> getStartReferencesProviders() {
        return Collections.unmodifiableList(startReferencesProviders);
    }

    /**
     * Sets the providers of references used as starting points for crawling.
     * Use this approach when references need to be provided
     * dynamically at launch time.
     * @param startReferencesProviders start references provider
     */
    public CrawlerConfig setStartReferencesProviders(List<ReferencesProvider> startReferencesProviders) {
        CollectionUtil.setAll(this.startReferencesProviders, startReferencesProviders);
        CollectionUtil.removeNulls(this.startReferencesProviders);
        return this;
    }

    /**
     * The exceptions we want to stop the crawler on.
     * By default the crawler will log exceptions from processing
     * a document and try to move on to the next without stopping.
     * Even if no exceptions are returned by this method,
     * the crawler can sometimes stop regardless if it cannot recover
     * safely from an exception.
     * To capture more exceptions, use a parent class (e.g., Exception
     * should catch them all).
     * @return exceptions that will stop the crawler when encountered
     */
    public List<Class<? extends Exception>> getStopOnExceptions() {
        return Collections.unmodifiableList(stopOnExceptions);
    }

    /**
     * Sets the exceptions we want to stop the crawler on.
     * By default the crawler will log exceptions from processing
     * a document and try to move on to the next without stopping.
     * Even if no exceptions are returned by this method,
     * the crawler can sometimes stop regardless if it cannot recover
     * safely from an exception.
     * To capture more exceptions, use a parent class (e.g., Exception
     * should catch them all).
     * @param stopOnExceptions exceptions that will stop the crawler when
     *         encountered
     */
    public CrawlerConfig setStopOnExceptions(List<Class<? extends Exception>> stopOnExceptions) {
        CollectionUtil.setAll(this.stopOnExceptions, stopOnExceptions);
        return this;
    }

    /**
     * Gets reference filters
     * @return reference filters
     */
    public List<ReferenceFilter> getReferenceFilters() {
        return Collections.unmodifiableList(referenceFilters);
    }

    /**
     * Sets reference filters.
     * @param referenceFilters the referenceFilters to set
     */
    public CrawlerConfig setReferenceFilters(List<ReferenceFilter> referenceFilters) {
        CollectionUtil.setAll(this.referenceFilters, referenceFilters);
        return this;
    }

    /**
     * Gets the document filters.
     * @return document filters
     */
    public List<DocumentFilter> getDocumentFilters() {
        return Collections.unmodifiableList(documentFilters);
    }

    /**
     * Sets document filters.
     * @param documentFilters document filters
     */
    public CrawlerConfig setDocumentFilters(List<DocumentFilter> documentFilters) {
        CollectionUtil.setAll(this.documentFilters, documentFilters);
        return this;
    }

    /**
     * Gets metadata filters.
     * @return metadata filters
     */
    public List<MetadataFilter> getMetadataFilters() {
        return Collections.unmodifiableList(metadataFilters);
    }

    /**
     * Sets metadata filters.
     * @param metadataFilters metadata filters
     */
    public CrawlerConfig setMetadataFilters(List<MetadataFilter> metadataFilters) {
        CollectionUtil.setAll(this.metadataFilters, metadataFilters);
        return this;
    }

    /**
     * Gets Committers responsible for persisting information
     * to a target location/repository.
     * @return list of Committers (never <code>null</code>)
     */
    public List<Committer> getCommitters() {
        return Collections.unmodifiableList(committers);
    }

    /**
     * Sets Committers responsible for persisting information
     * to a target location/repository.
     * @param committers list of Committers
     */
    public CrawlerConfig setCommitters(List<Committer> committers) {
        CollectionUtil.setAll(this.committers, committers);
        return this;
    }

    /**
     * Gets event listeners.
     * Those are considered additions to automatically
     * detected configuration objects implementing {@link EventListener}.
     * @return event listeners.
     */
    public List<EventListener<?>> getEventListeners() {
        return Collections.unmodifiableList(eventListeners);
    }

    /**
     * Sets event listeners.
     * Those are considered additions to automatically
     * detected configuration objects implementing {@link EventListener}.
     * @param eventListeners event listeners.
     */
    public CrawlerConfig setEventListeners(List<EventListener<?>> eventListeners) {
        CollectionUtil.setAll(this.eventListeners, eventListeners);
        return this;
    }

    /**
     * Adds event listeners.
     * Those are considered additions to automatically
     * detected configuration objects implementing {@link EventListener}.
     * @param eventListeners event listeners.
     */
    public void addEventListeners(List<EventListener<?>> eventListeners) {
        this.eventListeners.addAll(eventListeners);
    }

    /**
     * Adds an event listener.
     * Those are considered additions to automatically
     * detected configuration objects implementing {@link EventListener}.
     * @param eventListener event listener.
     */
    public void addEventListener(EventListener<?> eventListener) {
        eventListeners.add(eventListener);
    }

    /**
     * Clears all event listeners. The automatically
     * detected configuration objects implementing {@link EventListener}
     * are not cleared.
     */
    public void clearEventListeners() {
        eventListeners.clear();
    }

    /**
     * Gets pre-import processors.
     * @return pre-import processors
     */
    public List<DocumentProcessor> getPreImportProcessors() {
        return Collections.unmodifiableList(preImportProcessors);
    }

    /**
     * Sets pre-import processors.
     * @param preImportProcessors pre-import processors
     */
    @JsonIgnore
    public CrawlerConfig setPreImportProcessors(DocumentProcessor... preImportProcessors) {
        setPreImportProcessors(Arrays.asList(preImportProcessors));
        return this;
    }

    /**
     * Sets pre-import processors.
     * @param preImportProcessors pre-import processors
     */
    public CrawlerConfig setPreImportProcessors(List<DocumentProcessor> preImportProcessors) {
        CollectionUtil.setAll(this.preImportProcessors, preImportProcessors);
        CollectionUtil.removeNulls(this.preImportProcessors);
        return this;
    }

    /**
     * Gets post-import processors.
     * @return post-import processors
     */
    public List<DocumentProcessor> getPostImportProcessors() {
        return Collections.unmodifiableList(postImportProcessors);
    }

    /**
     * Sets post-import processors.
     * @param postImportProcessors post-import processors
     */
    @JsonIgnore
    public CrawlerConfig setPostImportProcessors(DocumentProcessor... postImportProcessors) {
        setPostImportProcessors(Arrays.asList(postImportProcessors));
        return this;
    }

    /**
     * Sets post-import processors.
     * @param postImportProcessors post-import processors
     */
    public CrawlerConfig setPostImportProcessors(List<DocumentProcessor> postImportProcessors) {
        CollectionUtil.setAll(this.postImportProcessors, postImportProcessors);
        CollectionUtil.removeNulls(this.postImportProcessors);
        return this;
    }

    /**
     * One or more fetchers responsible for pulling documents and document
     * metadata associated with a reference from a source.
     * When more than one are configured and for each documents, fetchers will
     * be invoked in their defined order, until the first one that accepts and
     * successfully process a reference (others are not invoked).
     * @return one or more fetchers
     */
    public List<Fetcher<?, ?>> getFetchers() {
        return Collections.unmodifiableList(fetchers);
    }

    /**
     * One or more fetchers responsible for pulling documents and document
     * metadata associated with a reference from a source.
     * When more than one are configured and for each documents, fetchers will
     * be invoked in their defined order, until the first one that accepts and
     * successfully process a reference (others are not invoked).
     * @param fetchers one or more fetchers
     */
    public CrawlerConfig setFetchers(List<Fetcher<?, ?>> fetchers) {
        CollectionUtil.setAll(this.fetchers, fetchers);
        return this;
    }

    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig() {
    }

    /**
     * The crawler unique identifier.
     * Using usual names is perfectly fine (non-alphanumeric characters are OK).
     * It is important for this crawler ID to be unique amongst your
     * crawlers in the same crawl session. On top of avoiding conflicts,
     * it facilitates integration with different systems and facilitates
     * tracking.
     * @return unique identifier
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public String getId() {
        return this.id;
    }

    /**
     * Whether the start references should be loaded asynchronously. When
     * <code>true</code>, the crawler will start processing the start
     * references in a separate thread as they are added to the queue
     * (as opposed to wait for queue initialization to be complete).
     * While this may speed up crawling, it may have an unexpected effect on
     * accuracy of {@link CrawlDocMetadata#DEPTH}. Use of this option is only
     * recommended when start references take a significant time to load.
     * @return <code>true</code> if initialized asynchronously
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public boolean isStartReferencesAsync() {
        return this.startReferencesAsync;
    }

    /**
     * The maximum number of threads a crawler can use. Default is two.
     * @return number of threads
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public int getNumThreads() {
        return this.numThreads;
    }

    /**
     * <p>
     * The maximum number of documents that can be processed before stopping.
     * Not all processed documents make it to your Committers
     * as some can be rejected.
     * </p>
     * <p>
     * In multi-threaded or clustered environments, the actual number
     * of documents processed may be a bit higher than the specified
     * maximum due to concurrency.
     * Upon reaching the configured maximum, the crawler will finish with
     * its documents actively being processed before stopping.
     * </p>
     * <p>
     * Reaching the maximum value does not terminate the crawl session but
     * rather pauses it.  On next run, the crawler will resume the same session,
     * processing an additional number of documents up to the maximum
     * specified.
     * This maximum allows crawling one or more sources
     * in chunks, processing a maximum number of documents each time.
     * When the session fully completes, the next run will start a new
     * crawl session. To prevent resuming an partial crawl session,
     * explicitly clean the crawl session.
     * </p>
     * <p>
     * Default is -1 (unlimited).
     * </p>
     * @return maximum number of documents that can be processed
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public int getMaxDocuments() {
        return this.maxDocuments;
    }

    /**
     * The maximum depth the crawler should go. The exact definition of depth
     * is crawler-specific. Examples: levels of sub-directories,
     * number of URL clicks to reach a page, etc. Refer to specific crawler
     * implementation for details. Default is -1 (unlimited).
     * @return maximum depth or -1 for unlimited depth
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public int getMaxDepth() {
        return this.maxDepth;
    }

    /**
     * The maximum amount of time to wait before shutting down an inactive
     * crawler thread.
     * A document taking longer to process than the specified timeout
     * when no other thread are available to process remaining documents
     * is also considered "inactive". Default is
     * {@value #DEFAULT_IDLE_PROCESSING_TIMEOUT}. A <code>null</code>
     * value means no timeouts.
     * @return time to wait for a document to be processed
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public Duration getIdleTimeout() {
        return this.idleTimeout;
    }

    /**
     * Minimum amount of time to wait between each logging of crawling
     * progress.
     * Default value is {@value #DEFAULT_MIN_PROGRESS_LOGGING_INTERVAL}.
     * A <code>null</code> value disables progress logging. Minimum value
     * is 1 second.
     * @return time to wait between each logging of crawling progress
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public Duration getMinProgressLoggingInterval() {
        return this.minProgressLoggingInterval;
    }

    /**
     * <p>The strategy to adopt when there are orphans.  Orphans are
     * references that were processed in a previous run, but were not in the
     * current run.  In other words, they are leftovers from a previous run
     * that were not re-encountered in the current.
     * </p><p>
     * Unless explicitly stated otherwise by an implementing class, the default
     * strategy is to <code>PROCESS</code> orphans.
     * Setting a <code>null</code> value is the same as setting
     * <code>IGNORE</code>.
     * </p><p>
     * <b>Be careful:</b> Setting the orphan strategy to <code>DELETE</code>
     * is NOT recommended in most cases. With some collectors, a temporary
     * failure such as a network outage or a web page timing out, may cause
     * some documents not to be crawled. When this happens, unreachable
     * documents would be considered "orphans" and be deleted while under
     * normal circumstances, they should be kept.  Re-processing them
     * (default), is usually the safest approach to confirm they still
     * exist before deleting or updating them.
     * </p>
     * @return orphans strategy
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public OrphansStrategy getOrphansStrategy() {
        return this.orphansStrategy;
    }

    /**
     * The crawl data store factory.
     * @return crawl data store factory.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public DataStoreEngine getDataStoreEngine() {
        return this.dataStoreEngine;
    }

    /**
     * The metadata checksummer.
     * Metadata checksum generation is disabled when <code>null</code>.
     * @return metadata checksummer or <code>null</code> when disabled
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public MetadataChecksummer getMetadataChecksummer() {
        return this.metadataChecksummer;
    }

    /**
     * The Importer module configuration.
     * @return Importer module configuration
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public ImporterConfig getImporterConfig() {
        return this.importerConfig;
    }

    /**
     * Whether to turn on deduplication based on metadata checksum.
     * To enable, {@link #getMetadataChecksummer()} must not return
     * <code>null</code>.
     * Not recommended unless you know for sure your metadata
     * checksum is acceptably unique.
     * @return whether to turn on metadata-based deduplication
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public boolean isMetadataDeduplicate() {
        return this.metadataDeduplicate;
    }

    /**
     * Whether to turn on deduplication based on document checksum.
     * To enable, {@link #getDocumentChecksummer()} must not return
     * <code>null</code>.
     * Not recommended unless you know for sure your document
     * checksum is acceptably unique.
     * @return whether to turn on document-based deduplication
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public boolean isDocumentDeduplicate() {
        return this.documentDeduplicate;
    }

    /**
     * The document checksummer.
     * Document checksum generation is disabled when <code>null</code>.
     * @return document checksummer or <code>null</code> when disabled
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public DocumentChecksummer getDocumentChecksummer() {
        return this.documentChecksummer;
    }

    /**
     * The spoiled state strategy resolver.
     * @return spoiled state strategy resolver
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public SpoiledReferenceStrategizer getSpoiledReferenceStrategizer() {
        return this.spoiledReferenceStrategizer;
    }

    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public FetchDirectiveSupport getMetadataFetchSupport() {
        return this.metadataFetchSupport;
    }

    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public FetchDirectiveSupport getDocumentFetchSupport() {
        return this.documentFetchSupport;
    }

    /**
     * The maximum number of times a fetcher will re-attempt fetching
     * a resource in case of failures.  Default is zero (won't retry).
     * @return maximum number of retries
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public int getFetchersMaxRetries() {
        return this.fetchersMaxRetries;
    }

    /**
     * How long to wait before a failing fetcher re-attempts fetching
     * a resource in case of failures (in milliseconds).
     * Default is zero (no delay).
     * @return retry delay
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public Duration getFetchersRetryDelay() {
        return this.fetchersRetryDelay;
    }

    /**
     * The crawler unique identifier.
     * Using usual names is perfectly fine (non-alphanumeric characters are OK).
     * It is important for this crawler ID to be unique amongst your
     * crawlers in the same crawl session. On top of avoiding conflicts,
     * it facilitates integration with different systems and facilitates
     * tracking.
     * @param id unique identifier
     * @return {@code this}.
     */
    @JsonProperty(required = true)
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setId(final String id) {
        this.id = id;
        return this;
    }

    /**
     * Whether the start references should be loaded asynchronously. When
     * <code>true</code>, the crawler will start processing the start
     * references in a separate thread as they are added to the queue
     * (as opposed to wait for queue initialization to be complete).
     * While this may speed up crawling, it may have an unexpected effect on
     * accuracy of {@link CrawlDocMetadata#DEPTH}. Use of this option is only
     * recommended when start references take a significant time to load.
     * @param startReferencesAsync <code>true</code> if initialized
     *     asynchronously
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setStartReferencesAsync(final boolean startReferencesAsync) {
        this.startReferencesAsync = startReferencesAsync;
        return this;
    }

    /**
     * The maximum number of threads a crawler can use. Default is two.
     * @param numThreads number of threads
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setNumThreads(final int numThreads) {
        this.numThreads = numThreads;
        return this;
    }

    /**
     * <p>
     * The maximum number of documents that can be processed before stopping.
     * Not all processed documents make it to your Committers
     * as some can be rejected.
     * </p>
     * <p>
     * In multi-threaded or clustered environments, the actual number
     * of documents processed may be a bit higher than the specified
     * maximum due to concurrency.
     * Upon reaching the configured maximum, the crawler will finish with
     * its documents actively being processed before stopping.
     * </p>
     * <p>
     * Reaching the maximum value does not terminate the crawl session but
     * rather pauses it.  On next run, the crawler will resume the same session,
     * processing an additional number of documents up to the maximum
     * specified.
     * This maximum allows crawling one or more sources
     * in chunks, processing a maximum number of documents each time.
     * When the session fully completes, the next run will start a new
     * crawl session. To prevent resuming an partial crawl session,
     * explicitly clean the crawl session.
     * </p>
     * <p>
     * Default is -1 (unlimited).
     * </p>
     * @param maxDocuments maximum number of documents that can be processed
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setMaxDocuments(final int maxDocuments) {
        this.maxDocuments = maxDocuments;
        return this;
    }

    /**
     * The maximum depth the crawler should go. The exact definition of depth
     * is crawler-specific. Examples: levels of sub-directories,
     * number of URL clicks to reach a page, etc. Refer to specific crawler
     * implementation for details. Default is -1 (unlimited).
     * @param maxDepth maximum depth or -1 for unlimited depth
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setMaxDepth(final int maxDepth) {
        this.maxDepth = maxDepth;
        return this;
    }

    /**
     * The maximum amount of time to wait before shutting down an inactive
     * crawler thread.
     * A document taking longer to process than the specified timeout
     * when no other thread are available to process remaining documents
     * is also considered "inactive". Default is
     * {@value #DEFAULT_IDLE_PROCESSING_TIMEOUT}. A <code>null</code>
     * value means no timeouts.
     * @param idleTimeout time to wait for a document to be processed
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setIdleTimeout(final Duration idleTimeout) {
        this.idleTimeout = idleTimeout;
        return this;
    }

    /**
     * Minimum amount of time to wait between each logging of crawling
     * progress.
     * Default value is {@value #DEFAULT_MIN_PROGRESS_LOGGING_INTERVAL}.
     * A <code>null</code> value disables progress logging. Minimum value
     * is 1 second.
     * @param minProgressLoggingInterval time to wait between each logging
     *     of crawling progress
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setMinProgressLoggingInterval(final Duration minProgressLoggingInterval) {
        this.minProgressLoggingInterval = minProgressLoggingInterval;
        return this;
    }

    /**
     * <p>The strategy to adopt when there are orphans.  Orphans are
     * references that were processed in a previous run, but were not in the
     * current run.  In other words, they are leftovers from a previous run
     * that were not re-encountered in the current.
     * </p><p>
     * Unless explicitly stated otherwise by an implementing class, the default
     * strategy is to <code>PROCESS</code> orphans.
     * Setting a <code>null</code> value is the same as setting
     * <code>IGNORE</code>.
     * </p><p>
     * <b>Be careful:</b> Setting the orphan strategy to <code>DELETE</code>
     * is NOT recommended in most cases. With some collectors, a temporary
     * failure such as a network outage or a web page timing out, may cause
     * some documents not to be crawled. When this happens, unreachable
     * documents would be considered "orphans" and be deleted while under
     * normal circumstances, they should be kept.  Re-processing them
     * (default), is usually the safest approach to confirm they still
     * exist before deleting or updating them.
     * </p>
     * @param orphansStrategy orphans strategy
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setOrphansStrategy(final OrphansStrategy orphansStrategy) {
        this.orphansStrategy = orphansStrategy;
        return this;
    }

    /**
     * The crawl data store factory.
     * @param dataStoreEngine crawl data store factory.
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setDataStoreEngine(final DataStoreEngine dataStoreEngine) {
        this.dataStoreEngine = dataStoreEngine;
        return this;
    }

    /**
     * The metadata checksummer.
     * Metadata checksum generation is disabled when <code>null</code>.
     * @param metadataChecksummer metadata checksummer
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setMetadataChecksummer(final MetadataChecksummer metadataChecksummer) {
        this.metadataChecksummer = metadataChecksummer;
        return this;
    }

    /**
     * The Importer module configuration.
     * @param importerConfig Importer module configuration
     * @return {@code this}.
     */
    @JsonProperty("importer")
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setImporterConfig(final ImporterConfig importerConfig) {
        this.importerConfig = importerConfig;
        return this;
    }

    /**
     * Whether to turn on deduplication based on metadata checksum.
     * To enable, {@link #getMetadataChecksummer()} must not return
     * <code>null</code>.
     * Not recommended unless you know for sure your metadata
     * checksum is acceptably unique.
     * @param metadataDeduplicate <code>true</code> to turn on
     *        metadata-based deduplication
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setMetadataDeduplicate(final boolean metadataDeduplicate) {
        this.metadataDeduplicate = metadataDeduplicate;
        return this;
    }

    /**
     * Whether to turn on deduplication based on document checksum.
     * To enable, {@link #getDocumentChecksummer()} must not return
     * <code>null</code>.
     * Not recommended unless you know for sure your document
     * checksum is acceptably unique.
     * @param documentDeduplicate <code>true</code> to turn on
     *        document-based deduplication
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setDocumentDeduplicate(final boolean documentDeduplicate) {
        this.documentDeduplicate = documentDeduplicate;
        return this;
    }

    /**
     * The document checksummer.
     * Document checksum generation is disabled when <code>null</code>.
     * @param documentChecksummer document checksummer
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setDocumentChecksummer(final DocumentChecksummer documentChecksummer) {
        this.documentChecksummer = documentChecksummer;
        return this;
    }

    /**
     * The spoiled state strategy resolver.
     * @param spoiledReferenceStrategizer spoiled state strategy resolver
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setSpoiledReferenceStrategizer(final SpoiledReferenceStrategizer spoiledReferenceStrategizer) {
        this.spoiledReferenceStrategizer = spoiledReferenceStrategizer;
        return this;
    }

    /**
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setMetadataFetchSupport(final FetchDirectiveSupport metadataFetchSupport) {
        this.metadataFetchSupport = metadataFetchSupport;
        return this;
    }

    /**
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setDocumentFetchSupport(final FetchDirectiveSupport documentFetchSupport) {
        this.documentFetchSupport = documentFetchSupport;
        return this;
    }

    /**
     * The maximum number of times a fetcher will re-attempt fetching
     * a resource in case of failures.  Default is zero (won't retry).
     * @param fetchersMaxRetries maximum number of retries
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setFetchersMaxRetries(final int fetchersMaxRetries) {
        this.fetchersMaxRetries = fetchersMaxRetries;
        return this;
    }

    /**
     * How long to wait before a failing fetcher re-attempts fetching
     * a resource in case of failures (in milliseconds).
     * Default is zero (no delay).
     * @param fetchersRetryDelay retry delay
     * @return {@code this}.
     */
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public CrawlerConfig setFetchersRetryDelay(final Duration fetchersRetryDelay) {
        this.fetchersRetryDelay = fetchersRetryDelay;
        return this;
    }

    @java.lang.Override
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public boolean equals(final java.lang.Object o) {
        if (o == this) return true;
        if (!(o instanceof CrawlerConfig)) return false;
        final CrawlerConfig other = (CrawlerConfig) o;
        if (!other.canEqual((java.lang.Object) this)) return false;
        if (this.isStartReferencesAsync() != other.isStartReferencesAsync()) return false;
        if (this.getNumThreads() != other.getNumThreads()) return false;
        if (this.getMaxDocuments() != other.getMaxDocuments()) return false;
        if (this.getMaxDepth() != other.getMaxDepth()) return false;
        if (this.isMetadataDeduplicate() != other.isMetadataDeduplicate()) return false;
        if (this.isDocumentDeduplicate() != other.isDocumentDeduplicate()) return false;
        if (this.getFetchersMaxRetries() != other.getFetchersMaxRetries()) return false;
        final java.lang.Object this$id = this.getId();
        final java.lang.Object other$id = other.getId();
        if (this$id == null ? other$id != null : !this$id.equals(other$id)) return false;
        final java.lang.Object this$startReferences = this.getStartReferences();
        final java.lang.Object other$startReferences = other.getStartReferences();
        if (this$startReferences == null ? other$startReferences != null : !this$startReferences.equals(other$startReferences)) return false;
        final java.lang.Object this$startReferencesFiles = this.getStartReferencesFiles();
        final java.lang.Object other$startReferencesFiles = other.getStartReferencesFiles();
        if (this$startReferencesFiles == null ? other$startReferencesFiles != null : !this$startReferencesFiles.equals(other$startReferencesFiles)) return false;
        final java.lang.Object this$startReferencesProviders = this.getStartReferencesProviders();
        final java.lang.Object other$startReferencesProviders = other.getStartReferencesProviders();
        if (this$startReferencesProviders == null ? other$startReferencesProviders != null : !this$startReferencesProviders.equals(other$startReferencesProviders)) return false;
        final java.lang.Object this$idleTimeout = this.getIdleTimeout();
        final java.lang.Object other$idleTimeout = other.getIdleTimeout();
        if (this$idleTimeout == null ? other$idleTimeout != null : !this$idleTimeout.equals(other$idleTimeout)) return false;
        final java.lang.Object this$minProgressLoggingInterval = this.getMinProgressLoggingInterval();
        final java.lang.Object other$minProgressLoggingInterval = other.getMinProgressLoggingInterval();
        if (this$minProgressLoggingInterval == null ? other$minProgressLoggingInterval != null : !this$minProgressLoggingInterval.equals(other$minProgressLoggingInterval)) return false;
        final java.lang.Object this$orphansStrategy = this.getOrphansStrategy();
        final java.lang.Object other$orphansStrategy = other.getOrphansStrategy();
        if (this$orphansStrategy == null ? other$orphansStrategy != null : !this$orphansStrategy.equals(other$orphansStrategy)) return false;
        final java.lang.Object this$stopOnExceptions = this.getStopOnExceptions();
        final java.lang.Object other$stopOnExceptions = other.getStopOnExceptions();
        if (this$stopOnExceptions == null ? other$stopOnExceptions != null : !this$stopOnExceptions.equals(other$stopOnExceptions)) return false;
        final java.lang.Object this$dataStoreEngine = this.getDataStoreEngine();
        final java.lang.Object other$dataStoreEngine = other.getDataStoreEngine();
        if (this$dataStoreEngine == null ? other$dataStoreEngine != null : !this$dataStoreEngine.equals(other$dataStoreEngine)) return false;
        final java.lang.Object this$referenceFilters = this.getReferenceFilters();
        final java.lang.Object other$referenceFilters = other.getReferenceFilters();
        if (this$referenceFilters == null ? other$referenceFilters != null : !this$referenceFilters.equals(other$referenceFilters)) return false;
        final java.lang.Object this$metadataFilters = this.getMetadataFilters();
        final java.lang.Object other$metadataFilters = other.getMetadataFilters();
        if (this$metadataFilters == null ? other$metadataFilters != null : !this$metadataFilters.equals(other$metadataFilters)) return false;
        final java.lang.Object this$documentFilters = this.getDocumentFilters();
        final java.lang.Object other$documentFilters = other.getDocumentFilters();
        if (this$documentFilters == null ? other$documentFilters != null : !this$documentFilters.equals(other$documentFilters)) return false;
        final java.lang.Object this$preImportProcessors = this.getPreImportProcessors();
        final java.lang.Object other$preImportProcessors = other.getPreImportProcessors();
        if (this$preImportProcessors == null ? other$preImportProcessors != null : !this$preImportProcessors.equals(other$preImportProcessors)) return false;
        final java.lang.Object this$postImportProcessors = this.getPostImportProcessors();
        final java.lang.Object other$postImportProcessors = other.getPostImportProcessors();
        if (this$postImportProcessors == null ? other$postImportProcessors != null : !this$postImportProcessors.equals(other$postImportProcessors)) return false;
        final java.lang.Object this$metadataChecksummer = this.getMetadataChecksummer();
        final java.lang.Object other$metadataChecksummer = other.getMetadataChecksummer();
        if (this$metadataChecksummer == null ? other$metadataChecksummer != null : !this$metadataChecksummer.equals(other$metadataChecksummer)) return false;
        final java.lang.Object this$importerConfig = this.getImporterConfig();
        final java.lang.Object other$importerConfig = other.getImporterConfig();
        if (this$importerConfig == null ? other$importerConfig != null : !this$importerConfig.equals(other$importerConfig)) return false;
        final java.lang.Object this$committers = this.getCommitters();
        final java.lang.Object other$committers = other.getCommitters();
        if (this$committers == null ? other$committers != null : !this$committers.equals(other$committers)) return false;
        final java.lang.Object this$documentChecksummer = this.getDocumentChecksummer();
        final java.lang.Object other$documentChecksummer = other.getDocumentChecksummer();
        if (this$documentChecksummer == null ? other$documentChecksummer != null : !this$documentChecksummer.equals(other$documentChecksummer)) return false;
        final java.lang.Object this$spoiledReferenceStrategizer = this.getSpoiledReferenceStrategizer();
        final java.lang.Object other$spoiledReferenceStrategizer = other.getSpoiledReferenceStrategizer();
        if (this$spoiledReferenceStrategizer == null ? other$spoiledReferenceStrategizer != null : !this$spoiledReferenceStrategizer.equals(other$spoiledReferenceStrategizer)) return false;
        final java.lang.Object this$eventListeners = this.getEventListeners();
        final java.lang.Object other$eventListeners = other.getEventListeners();
        if (this$eventListeners == null ? other$eventListeners != null : !this$eventListeners.equals(other$eventListeners)) return false;
        final java.lang.Object this$metadataFetchSupport = this.getMetadataFetchSupport();
        final java.lang.Object other$metadataFetchSupport = other.getMetadataFetchSupport();
        if (this$metadataFetchSupport == null ? other$metadataFetchSupport != null : !this$metadataFetchSupport.equals(other$metadataFetchSupport)) return false;
        final java.lang.Object this$documentFetchSupport = this.getDocumentFetchSupport();
        final java.lang.Object other$documentFetchSupport = other.getDocumentFetchSupport();
        if (this$documentFetchSupport == null ? other$documentFetchSupport != null : !this$documentFetchSupport.equals(other$documentFetchSupport)) return false;
        final java.lang.Object this$fetchers = this.getFetchers();
        final java.lang.Object other$fetchers = other.getFetchers();
        if (this$fetchers == null ? other$fetchers != null : !this$fetchers.equals(other$fetchers)) return false;
        final java.lang.Object this$fetchersRetryDelay = this.getFetchersRetryDelay();
        final java.lang.Object other$fetchersRetryDelay = other.getFetchersRetryDelay();
        if (this$fetchersRetryDelay == null ? other$fetchersRetryDelay != null : !this$fetchersRetryDelay.equals(other$fetchersRetryDelay)) return false;
        return true;
    }

    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    protected boolean canEqual(final java.lang.Object other) {
        return other instanceof CrawlerConfig;
    }

    @java.lang.Override
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public int hashCode() {
        final int PRIME = 59;
        int result = 1;
        result = result * PRIME + (this.isStartReferencesAsync() ? 79 : 97);
        result = result * PRIME + this.getNumThreads();
        result = result * PRIME + this.getMaxDocuments();
        result = result * PRIME + this.getMaxDepth();
        result = result * PRIME + (this.isMetadataDeduplicate() ? 79 : 97);
        result = result * PRIME + (this.isDocumentDeduplicate() ? 79 : 97);
        result = result * PRIME + this.getFetchersMaxRetries();
        final java.lang.Object $id = this.getId();
        result = result * PRIME + ($id == null ? 43 : $id.hashCode());
        final java.lang.Object $startReferences = this.getStartReferences();
        result = result * PRIME + ($startReferences == null ? 43 : $startReferences.hashCode());
        final java.lang.Object $startReferencesFiles = this.getStartReferencesFiles();
        result = result * PRIME + ($startReferencesFiles == null ? 43 : $startReferencesFiles.hashCode());
        final java.lang.Object $startReferencesProviders = this.getStartReferencesProviders();
        result = result * PRIME + ($startReferencesProviders == null ? 43 : $startReferencesProviders.hashCode());
        final java.lang.Object $idleTimeout = this.getIdleTimeout();
        result = result * PRIME + ($idleTimeout == null ? 43 : $idleTimeout.hashCode());
        final java.lang.Object $minProgressLoggingInterval = this.getMinProgressLoggingInterval();
        result = result * PRIME + ($minProgressLoggingInterval == null ? 43 : $minProgressLoggingInterval.hashCode());
        final java.lang.Object $orphansStrategy = this.getOrphansStrategy();
        result = result * PRIME + ($orphansStrategy == null ? 43 : $orphansStrategy.hashCode());
        final java.lang.Object $stopOnExceptions = this.getStopOnExceptions();
        result = result * PRIME + ($stopOnExceptions == null ? 43 : $stopOnExceptions.hashCode());
        final java.lang.Object $dataStoreEngine = this.getDataStoreEngine();
        result = result * PRIME + ($dataStoreEngine == null ? 43 : $dataStoreEngine.hashCode());
        final java.lang.Object $referenceFilters = this.getReferenceFilters();
        result = result * PRIME + ($referenceFilters == null ? 43 : $referenceFilters.hashCode());
        final java.lang.Object $metadataFilters = this.getMetadataFilters();
        result = result * PRIME + ($metadataFilters == null ? 43 : $metadataFilters.hashCode());
        final java.lang.Object $documentFilters = this.getDocumentFilters();
        result = result * PRIME + ($documentFilters == null ? 43 : $documentFilters.hashCode());
        final java.lang.Object $preImportProcessors = this.getPreImportProcessors();
        result = result * PRIME + ($preImportProcessors == null ? 43 : $preImportProcessors.hashCode());
        final java.lang.Object $postImportProcessors = this.getPostImportProcessors();
        result = result * PRIME + ($postImportProcessors == null ? 43 : $postImportProcessors.hashCode());
        final java.lang.Object $metadataChecksummer = this.getMetadataChecksummer();
        result = result * PRIME + ($metadataChecksummer == null ? 43 : $metadataChecksummer.hashCode());
        final java.lang.Object $importerConfig = this.getImporterConfig();
        result = result * PRIME + ($importerConfig == null ? 43 : $importerConfig.hashCode());
        final java.lang.Object $committers = this.getCommitters();
        result = result * PRIME + ($committers == null ? 43 : $committers.hashCode());
        final java.lang.Object $documentChecksummer = this.getDocumentChecksummer();
        result = result * PRIME + ($documentChecksummer == null ? 43 : $documentChecksummer.hashCode());
        final java.lang.Object $spoiledReferenceStrategizer = this.getSpoiledReferenceStrategizer();
        result = result * PRIME + ($spoiledReferenceStrategizer == null ? 43 : $spoiledReferenceStrategizer.hashCode());
        final java.lang.Object $eventListeners = this.getEventListeners();
        result = result * PRIME + ($eventListeners == null ? 43 : $eventListeners.hashCode());
        final java.lang.Object $metadataFetchSupport = this.getMetadataFetchSupport();
        result = result * PRIME + ($metadataFetchSupport == null ? 43 : $metadataFetchSupport.hashCode());
        final java.lang.Object $documentFetchSupport = this.getDocumentFetchSupport();
        result = result * PRIME + ($documentFetchSupport == null ? 43 : $documentFetchSupport.hashCode());
        final java.lang.Object $fetchers = this.getFetchers();
        result = result * PRIME + ($fetchers == null ? 43 : $fetchers.hashCode());
        final java.lang.Object $fetchersRetryDelay = this.getFetchersRetryDelay();
        result = result * PRIME + ($fetchersRetryDelay == null ? 43 : $fetchersRetryDelay.hashCode());
        return result;
    }

    @java.lang.Override
    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public java.lang.String toString() {
        return "CrawlerConfig(id=" + this.getId() + ", startReferences=" + this.getStartReferences() + ", startReferencesFiles=" + this.getStartReferencesFiles() + ", startReferencesProviders=" + this.getStartReferencesProviders() + ", startReferencesAsync=" + this.isStartReferencesAsync() + ", numThreads=" + this.getNumThreads() + ", maxDocuments=" + this.getMaxDocuments() + ", maxDepth=" + this.getMaxDepth() + ", idleTimeout=" + this.getIdleTimeout() + ", minProgressLoggingInterval=" + this.getMinProgressLoggingInterval() + ", orphansStrategy=" + this.getOrphansStrategy() + ", stopOnExceptions=" + this.getStopOnExceptions() + ", dataStoreEngine=" + this.getDataStoreEngine() + ", referenceFilters=" + this.getReferenceFilters() + ", metadataFilters=" + this.getMetadataFilters() + ", documentFilters=" + this.getDocumentFilters() + ", preImportProcessors=" + this.getPreImportProcessors() + ", postImportProcessors=" + this.getPostImportProcessors() + ", metadataChecksummer=" + this.getMetadataChecksummer() + ", importerConfig=" + this.getImporterConfig() + ", committers=" + this.getCommitters() + ", metadataDeduplicate=" + this.isMetadataDeduplicate() + ", documentDeduplicate=" + this.isDocumentDeduplicate() + ", documentChecksummer=" + this.getDocumentChecksummer() + ", spoiledReferenceStrategizer=" + this.getSpoiledReferenceStrategizer() + ", eventListeners=" + this.getEventListeners() + ", metadataFetchSupport=" + this.getMetadataFetchSupport() + ", documentFetchSupport=" + this.getDocumentFetchSupport() + ", fetchers=" + this.getFetchers() + ", fetchersMaxRetries=" + this.getFetchersMaxRetries() + ", fetchersRetryDelay=" + this.getFetchersRetryDelay() + ")";
    }


    @java.lang.SuppressWarnings("all")
    @lombok.Generated
    public static final class Fields {
        public static final java.lang.String id = "id";
        public static final java.lang.String startReferences = "startReferences";
        public static final java.lang.String startReferencesFiles = "startReferencesFiles";
        public static final java.lang.String startReferencesProviders = "startReferencesProviders";
        public static final java.lang.String startReferencesAsync = "startReferencesAsync";
        public static final java.lang.String numThreads = "numThreads";
        public static final java.lang.String maxDocuments = "maxDocuments";
        public static final java.lang.String maxDepth = "maxDepth";
        public static final java.lang.String idleTimeout = "idleTimeout";
        public static final java.lang.String minProgressLoggingInterval = "minProgressLoggingInterval";
        public static final java.lang.String orphansStrategy = "orphansStrategy";
        public static final java.lang.String stopOnExceptions = "stopOnExceptions";
        public static final java.lang.String dataStoreEngine = "dataStoreEngine";
        public static final java.lang.String referenceFilters = "referenceFilters";
        public static final java.lang.String metadataFilters = "metadataFilters";
        public static final java.lang.String documentFilters = "documentFilters";
        public static final java.lang.String preImportProcessors = "preImportProcessors";
        public static final java.lang.String postImportProcessors = "postImportProcessors";
        public static final java.lang.String metadataChecksummer = "metadataChecksummer";
        public static final java.lang.String importerConfig = "importerConfig";
        public static final java.lang.String committers = "committers";
        public static final java.lang.String metadataDeduplicate = "metadataDeduplicate";
        public static final java.lang.String documentDeduplicate = "documentDeduplicate";
        public static final java.lang.String documentChecksummer = "documentChecksummer";
        public static final java.lang.String spoiledReferenceStrategizer = "spoiledReferenceStrategizer";
        public static final java.lang.String eventListeners = "eventListeners";
        public static final java.lang.String metadataFetchSupport = "metadataFetchSupport";
        public static final java.lang.String documentFetchSupport = "documentFetchSupport";
        public static final java.lang.String fetchers = "fetchers";
        public static final java.lang.String fetchersMaxRetries = "fetchersMaxRetries";
        public static final java.lang.String fetchersRetryDelay = "fetchersRetryDelay";
    }
//    //--- XML Persist ----------------------------------------------------------
//
//    @Override
//    public void saveToXML(XML xml) {
//        xml.setAttribute(Fields.id, id);
//
//        var startXML = xml.addElement("start")
//                .setAttribute("async", startReferencesAsync);
//        startXML.addElementList("ref", startReferences);
//        startXML.addElementList("refsFile", startReferencesFiles);
//        startXML.addElementList("provider", startReferencesProviders);
//
//        xml.addElement(Fields.numThreads, numThreads);
//        xml.addElement(Fields.maxDocuments, maxDocuments);
//        xml.addElement(Fields.maxDepth, maxDepth);
//        xml.addElement(Fields.idleTimeout, idleTimeout);
//        xml.addElement(Fields.minProgressLoggingInterval,
//                minProgressLoggingInterval);
//        xml.addElementList(
//                Fields.stopOnExceptions, "exception", stopOnExceptions);
//        xml.addElement(Fields.orphansStrategy, orphansStrategy);
//        xml.addElement(Fields.dataStoreEngine, dataStoreEngine);
//        xml.addElementList(Fields.referenceFilters, "filter", referenceFilters);
//        xml.addElementList(Fields.metadataFilters, "filter", metadataFilters);
//        xml.addElementList(Fields.documentFilters, "filter", documentFilters);
//        if (importerConfig != null) {
//            xml.addElement("importer", importerConfig);
//        }
//        xml.addElementList(Fields.committers, "committer", committers);
//        xml.addElement(Fields.metadataChecksummer, metadataChecksummer);
//        xml.addElement(Fields.metadataDeduplicate, metadataDeduplicate);
//        xml.addElement(Fields.documentChecksummer, documentChecksummer);
//        xml.addElement(Fields.documentDeduplicate, documentDeduplicate);
//        xml.addElement(Fields.spoiledReferenceStrategizer,
//                spoiledReferenceStrategizer);
//
//        xml.addElementList(Fields.eventListeners, "listener", eventListeners);
//
//        xml.addElement(Fields.metadataFetchSupport, metadataFetchSupport);
//        xml.addElement(Fields.documentFetchSupport, documentFetchSupport);
//
//        xml.addElementList(
//                Fields.preImportProcessors, "processor", preImportProcessors);
//        xml.addElementList(
//                Fields.postImportProcessors, "processor", postImportProcessors);
//
//        xml.addElement(Fields.fetchers)
//            .setAttribute("maxRetries", fetchersMaxRetries)
//            .setAttribute("retryDelay", fetchersRetryDelay)
//            .addElementList("fetcher", fetchers);
//    }
//
//    @Override
//    public void loadFromXML(XML xml) {
//        setId(xml.getString(XPathUtil.attr(Fields.id), id));
//
//        setStartReferences(xml.getStringList("start/ref", startReferences));
//        setStartReferencesFiles(
//                xml.getPathList("start/refsFile", startReferencesFiles));
//        setStartReferencesProviders(
//                xml.getObjectListImpl(ReferencesProvider.class,
//                "start/provider", startReferencesProviders));
//        setStartReferencesAsync(
//                xml.getBoolean("start/@async", startReferencesAsync));
//
//        setNumThreads(xml.getInteger(Fields.numThreads, numThreads));
//        setMaxDocuments(xml.getInteger(Fields.maxDocuments, maxDocuments));
//        setMaxDepth(xml.getInteger(Fields.maxDepth, maxDepth));
//        setOrphansStrategy(xml.getEnum(Fields.orphansStrategy,
//                OrphansStrategy.class, orphansStrategy));
//        setIdleTimeout(xml.getDuration(Fields.idleTimeout, idleTimeout));
//        setMinProgressLoggingInterval(xml.getDuration(
//                Fields.minProgressLoggingInterval, minProgressLoggingInterval));
//        setStopOnExceptions(xml.getClassList(
//                "stopOnExceptions/exception", stopOnExceptions));
//        setReferenceFilters(xml.getObjectListImpl(ReferenceFilter.class,
//                "referenceFilters/filter", referenceFilters));
//        setMetadataFilters(xml.getObjectListImpl(MetadataFilter.class,
//                "metadataFilters/filter", metadataFilters));
//        setDocumentFilters(xml.getObjectListImpl(DocumentFilter.class,
//                "documentFilters/filter", documentFilters));
//
//        var importerXML = xml.getXML("importer");
//        if (importerXML != null) {
//            var cfg = new ImporterConfig();
//            importerXML.populate(cfg);
//            setImporterConfig(cfg);
//            //MAYBE handle ignore errors
//        } else if (getImporterConfig() == null) {
//            setImporterConfig(new ImporterConfig());
//        }
//
//        setDataStoreEngine(xml.getObjectImpl(
//                DataStoreEngine.class, "dataStoreEngine", dataStoreEngine));
//        setCommitters(xml.getObjectListImpl(Committer.class,
//                "committers/committer", committers));
//        setMetadataChecksummer(xml.getObjectImpl(MetadataChecksummer.class,
//                Fields.metadataChecksummer, metadataChecksummer));
//        setMetadataDeduplicate(xml.getBoolean("metadataDeduplicate",
//                metadataDeduplicate));
//        setDocumentChecksummer(xml.getObjectImpl(DocumentChecksummer.class,
//                Fields.documentChecksummer, documentChecksummer));
//        setDocumentDeduplicate(xml.getBoolean("documentDeduplicate",
//                documentDeduplicate));
//        setSpoiledReferenceStrategizer(xml.getObjectImpl(
//                SpoiledReferenceStrategizer.class,
//                Fields.spoiledReferenceStrategizer,
//                spoiledReferenceStrategizer));
//
//        setEventListeners(xml.getObjectListImpl(EventListener.class,
//                "eventListeners/listener", eventListeners));
//
//        setMetadataFetchSupport(xml.getEnum(
//                Fields.metadataFetchSupport,
//                FetchDirectiveSupport.class,
//                metadataFetchSupport));
//        setDocumentFetchSupport(xml.getEnum(
//                Fields.documentFetchSupport,
//                FetchDirectiveSupport.class,
//                documentFetchSupport));
//
//        setPreImportProcessors(xml.getObjectListImpl(DocumentProcessor.class,
//                "preImportProcessors/processor", preImportProcessors));
//        setPostImportProcessors(xml.getObjectListImpl(DocumentProcessor.class,
//                "postImportProcessors/processor", postImportProcessors));
//
//        setFetchers(xml.getObjectListImpl(
//                Fetcher.class, "fetchers/fetcher", fetchers));
//        setFetchersMaxRetries(xml.getInteger(
//                "fetchers/@maxRetries", fetchersMaxRetries));
//        setFetchersRetryDelay(xml.getDurationMillis(
//                "fetchers/@retryDelay", fetchersRetryDelay));
//    }
}
